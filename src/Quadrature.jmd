# Interpolation and Gaussian quadrature


Consider integration
$$
\int_a^b f(x) w(x) {\rm d}x.
$$
For periodic integration we approximated (using the Trapezium rule) an integral by a sum.
We can think of it as a weighted sum:
$$
{1 \over 2œÄ} \int_0^{2œÄ} f(Œ∏) {\rm d} Œ∏ ‚âà  ‚àë_{j=0}^{n-1} w_j f(Œ∏_j)
$$
where $w_j = 1/n$. Replacing an integral by a weighted sum is a known as a _quadrature_ rule.
This quadrature rule had several important properties:
1. It was _exact_ for integrating trigonometric polynomials with 2n-1 coefficients
$$
p(Œ∏) = \sum_{k=1-n}^{n-1} pÃÇ_k \exp({\rm i}k Œ∏)
$$
as seen by the formula
$$
‚àë_{j=0}^{n-1} w_j f(Œ∏_j) = pÃÇ_0^n = ‚Ä¶ + pÃÇ_{n-1} + pÃÇ_0 + pÃÇ_n + ‚ãØ = pÃÇ_0 = {1 \over 2œÄ} \int_0^{2œÄ} p(Œ∏) {\rm d} Œ∏
$$
2. It exactly recovered the coefficients ($pÃÇ_k^n = pÃÇ_k$) for expansions of trigonometric polynomials with $n$ coeffiicents:
$$
p(Œ∏) = \sum_{k=-‚åà(n-1)/2‚åâ}^{‚åä(n-1)/2‚åã} pÃÇ_k \exp({\rm i}k Œ∏)
$$
3. It converged fast for smooth, periodic functions $f$.

In this section we consider other quadrature rules
$$
\int_a^b f(x) w(x) {\rm d}x ‚âà \sum_{j=1}^n w_j f(x_j)
$$
We want to choose $w_j$ and $x_j$ so that the following properties are satisfied:
1. It is _exact_ for integrating polynomials up to degree $2n-1$:
$$
p(Œ∏) = \sum_{k=0}^{2n-1} c_k q_k(x)
$$
2. It exactly recovers the coefficients for expansions:
$$
p(Œ∏) = \sum_{k=0}^{n-1} c_k q_k(x)
$$
3. It converges fast for smooth functions $f$.
We will focus on properties (1) and (2) as property (3) is more involved.



1. Polynomial Interpolation: we describe how to interpolate a function by a polynomial.
2. Interpolatory quadrature rule: polynomial interpolation leads naturally to ways to integrate
functions, but onely realisable in the simplest cases.
3. Truncated Jacobi matrices: we see that truncated Jacobi matrices are diagonalisable
in terms of orthogonal polynomials and their zeros. 
3. Gaussian quadrature: Using roots of orthogonal polynomials and truncated Jacobi matrices 
leads naturally to an efficiently
computable interpolatory quadrature rule. The _miracle_ is its exact for twice as many polynomials as
expected.



## 1. Polynomial Interpolation

We already saw a special case of polynomial interpolation, where we saw that the polynomial
$$
f(z) ‚âà ‚àë_{k=0}^{n-1} fÃÇ_k^n z^k
$$
equaled $f$ at evenly spaced points on the unit circle: ${\rm e}^{{\rm i} 2œÄ j/n}$. 
But here we consider the following:

**Definition (interpolatory polynomial)** Given $n$ distinct points $x_1,‚Ä¶,x_n ‚àà ‚Ñù$ 
and $n$ _samples_ $f_1,‚Ä¶,f_n ‚àà ‚Ñù$, a degree $n-1$
_interpolatory polynomial_ $p(x)$ satisfies
$$
p(x_j) = f_j
$$

The easiest way to solve this problem is to invert the Vandermonde system:

**Definition (Vandermonde)** The _Vandermonde matrix_ associated with $n$ distinct points $x_1,‚Ä¶,x_n ‚àà ‚Ñù$
is the matrix
$$
V := \begin{bmatrix} 1 & x_1 & ‚ãØ & x_1^{n-1} \\
                    ‚ãÆ & ‚ãÆ & ‚ã± & ‚ãÆ \\
                    1 & x_n & ‚ãØ & x_n^{n-1}
                    \end{bmatrix}
$$

**Proposition (interpolatory polynomial uniqueness)** 
The interpolatory polynomial is unique, and the Vandermonde matrix is invertible.

**Proof**
Suppose $p$ and $pÃÉ$ are both interpolatory polynomials. Then $p(x) - pÃÉ(x)$ vanishes at $n$ distinct points $x_j$. By the fundamental theorem of
algebra it must be zero, i.e., $p = pÃÉ$.

For the second part, if $V ùêú = 0$ for $ùêú ‚àà ‚Ñù$ then for $q(x) = c_1 + ‚ãØ + c_n x^{n-1}$ we have
$$
q(x_j) = ùêû_j^‚ä§ V ùêú = 0
$$
hence $q$ vanishes at $n$ distinct points and is therefore 0, i.e., $ùêú = 0$.

‚àé

Thus a quick-and-dirty way to to do interpolation is to invert the Vandermonde matrix
(which we saw in the least squares setting with more samples then coefficients):
```julia
using Plots, LinearAlgebra
f = x -> cos(3x)
n = 10

x = range(0, 1; length=n-1)# evenly spaced points (BAD for interpolation)
V = x .^ (0:n-1)' # Vandermonde matrix
c = V \ f.(x) # coefficients of interpolatory polynomial
p = x -> dot(c, x .^ (0:n-1))

g = range(0,1; length=1000) # plotting grid
plot(g, f.(g); label="function")
plot!(g, p.(g); label="interpolation")
```

But it turns out we can also construct the interpolatory polynomial directly.
We will use the following with equal $1$ at one grid point
and are zero at the others:

**Definition (Lagrange basis polynomial)** The _Lagrange basis polynomial_ is defined as
$$
‚Ñì_k(x) := ‚àè_{j ‚â† k} {x-x_j \over x_k - x_j} =  {(x-x_1) ‚ãØ(x-x_{k-1})(x-x_{k+1}) ‚ãØ (x-x_n) \over (x_k - x_1) ‚ãØ (x_k - x_{k-1}) (x_k - x_{k+1}) ‚ãØ (x_k - x_n)}
$$

Plugging in the grid points verifies the following:

**Proposition (delta interpolation)**
$$
‚Ñì_k(x_j) = Œ¥_{kj}
$$

We can use these to construct the interpolatory polynomial:

**Theorem (Lagrange interpolation)**
The unique  polynomial of degree at most $n-1$ that interpolates $f$ at $x_j$ is:
$$
p(x) = f(x_1) ‚Ñì_1(x) + ‚ãØ + f(x_n) ‚Ñì_n(x)
$$

**Proof**
Note that
$$
p(x_j) = ‚àë_{k=1}^n f(x_k) ‚Ñì_k(x_j) = f(x_k)
$$
so we just need to show it is unique. Suppose $pÃÉ(x)$ is a  polynomial
of degree at most $n-1$
that also interpolates $f$.


‚àé

**Example** We can interpolate $\exp(x)$ at the points $0,1,2$:
$$
p(x) = ‚Ñì_1(x) + {\rm e} ‚Ñì_2(x) + {\rm e}^2 ‚Ñì_3(x) = 
{(x - 1) (x-2) \over (-1)(-2)} + {\rm e} {x (x-2) \over (-1)} + 
{\rm e}^2 {x (x-1) \over 2} = (1/2 - {\rm e} +{\rm e}^2/2)x^2  
-  (-3/2 + 2 {\rm e}  - {\rm e}^2 /2)  x + 1
$$


**Remark** Interpolating at evenly spaced points is a really **bad** idea:
interpolation is inheritely ill-conditioned. 
The problem sheet asks you to explore
this experimentally.

## 2. Interpolatory quadrature rules

**Definition (interpolatory quadrature rule)** Given a set of points $x_1,‚Ä¶,x_n$
the interpolatory quadrature rule is:
$$
Œ£_n^{\rm i}[f] := ‚àë_{k=1}^n w_k f(x_k)
$$
where
$$
w_k := ‚à´_a^b ‚Ñì_k(x) w(x) {\rm d} x
$$


**Proposition (interpolatory quadrature is exact for polynomials)** 
Interpolatory quadrature is exact for all degree $n-1$ polynomials $p$:
$$
‚à´_a^b p(x) w(x) {\rm d}x = Œ£_n^{\rm i}[f]
$$

**Proof**
The result follows since, by uniqueness of interpolatory polynomial:
$$
p(x) = ‚àë_{k=1}^n p(x_k) ‚Ñì_k(x)
$$

‚àé


## 3. Roots of orthogonal polynomials and truncated Jacobi matrices

The key to property (1) is to use _roots (zeros) of $q_n(x)$_.

**Lemma** $q_n(x)$ has exactly $n$ distinct roots.

**Proof**

Suppose $x_1, ‚Ä¶,x_j$ are the roots where $q_n(x)$ changes sign, that is,
$$
q_n(x) = c_j (x-x_j) + O((x-x_j)^2)
$$
for $c_j ‚â† 0$. Then
$$
q_n(x) (x-x_1) ‚ãØ(x-x_j)
$$
does not change sign.
In other words:
$$
‚ü®q_n,(x-x_1) ‚ãØ(x-x_j) ‚ü© = \int_a^b q_n(x) (x-x_1) ‚ãØ(x-x_j) {\rm d} x ‚â† 0.
$$
This is only possible if $j = n$.

‚àé

**Definition (truncated Jacobi matrix)** Given a symmetric Jacobi matrix $X$,
(or the weight $w(x)$ whose orthonormal polynomials are associated with $X$)
 the _truncated Jacobi matrix_ is
$$
X_n := \begin{bmatrix} a_0 & b_0 \\
                         b_0 & ‚ã± & ‚ã± \\
                         & ‚ã± & a_{n-2} & b_{n-2} \\
                         && b_{n-2} & a_{n-1} \end{bmatrix} ‚àà ‚Ñù^{n √ó n}
$$



**Lemma (zeros)** The zeros $x_1, ‚Ä¶,x_n$ of $q_n(x)$ are the eigenvalues of the truncated Jacobi matrix $X_n$.
More precisely,
$$
X_n Q_n = Q_n \begin{bmatrix} x_1 \\ & ‚ã± \\ && x_n \end{bmatrix}
$$
for the orthogonal matrix
$$
Q_n = \begin{bmatrix}
q_0(x_1) & ‚ãØ & q_0(x_n) \\
‚ãÆ  & ‚ãØ & ‚ãÆ  \\
q_{n-1}(x_1) & ‚ãØ & q_{n-1}(x_n)
\end{bmatrix} \begin{bmatrix} Œ±_1^{-1} \\ & ‚ã± \\ && Œ±_n^{-1} \end{bmatrix}
$$
where $Œ±_k = \sqrt{q_0(x_k)^2 + ‚ãØ + q_{n-1}(x_k)^2}$.

**Proof**

We construct the eigenvector (noting $b_{n-1} p_n(x_j) = 0$):
$$
X_n \begin{bmatrix} p_0(x_j) \\ ‚ãÆ \\ p_{n-1}(x_j) \end{bmatrix} =
\begin{bmatrix} a_0 p_0(x_j) + b_0 p_1(x_j) \\
 b_0 p_0(x_j) + a_1 p_1(x_j) + b_1 p_2(x_j) \\
‚ãÆ \\
b_{n-3} p_{n-3}(x_j) + a_{n-2} p_{n-2}(x_j) + b_{n-2} p_{n-1}(x_j) \\
b_{n-2} p_{n-2}(x_j) + a_{n-1} p_{n-1}(x_j) + b_{n-1} p_n(x_j)
\end{bmatrix} = x_j \begin{bmatrix} p_0(x_j) \\
 p_1(x_j) \\
‚ãÆ \\
p_n(x_j)
\end{bmatrix}
$$
The result follows from normalising the eigenvectors. Since $X_n$ is symmetric
the eigenvector matrix is orthogonal.

‚àé



## 4. Gaussian quadrature

Gaussian quadrature is the interpolatory quadrature rule corresponding
to the grid $x_k$ defined as the roots of the orthogonal polynomial $q_n(x)$.
We shall see that it is exact for polynomials up to degree $2n-1$, i.e., double
the degree of other interpolatory quadrature rules from other grids.



**Definition (Gauss quadrature)** Given a weight $w(x)$, the Gauss quadrature rule is:
$$
‚à´_a^b f(x)w(x) {\rm d}x ‚âà \underbrace{‚àë_{k=1}^n w_k f(x_k)}_{Œ£_n^w[f]}
$$
where $x_1,‚Ä¶,x_n$ are the eigenvalues of $X_n$ and
$$
w_k = Q_n[k,1]^2 = {1 \over Œ±_k^2}
$$

In analogy to how Fourier series are orthogonal with respect to Trapezium rule,
Orthogonal polynomials are orthogonal with respect to Gaussian quadrature:

**Lemma (Discrete orthogonality)**
For $0 ‚â§¬†‚Ñì,m ‚â§¬†n-1$,
$$
Œ£_n^w[q_‚Ñì q_m] = Œ¥_{‚Ñìm}
$$

**Proof**
$$
Œ£_n^w[q_‚Ñì q_m] = ‚àë_{k=1}^n {q_‚Ñì(x_k) q_m(x_k) \over Œ±_k^2}
= \left[q_‚Ñì(x_1)/ Œ±_1 | ‚ãØ | {q_‚Ñì(x_n)/ Œ±_n}\right] 
\begin{bmatrix}
q_m(x_1)/Œ±_1 \\
‚ãÆ \\
q_m(x_n)/Œ±_n \end{bmatrix} = ùêû_‚Ñì Q_n Q_n^‚ä§ ùêû_m = Œ¥_{‚Ñìm}
$$

‚àé

Just as approximating Fourier coefficients using Trapezium rule gives a way of
interpolating at the grid, so does Gaussian quadrature:

**Theorem (interpolation via quadrature)**
$$
f_n(x) = ‚àë_{k=0}^{n-1} c_k^n q_k(x)\hbox{ for } c_k^n := Œ£_n^w[f q_k]
$$
interpolates $f(x)$ at the Gaussian quadrature points $x_1,‚Ä¶,x_n$.

**Proof**

Note that we can write:
$$
\begin{bmatrix}
c_0^n \\
‚ãÆ \\
c_{n-1}^n \end{bmatrix} = Q_n^w \begin{bmatrix}
f_1 \\
‚ãÆ \\
f_n \end{bmatrix}.
$$

Consider the Vandermonde-like matrix:
$$
VÃÉ = \begin{bmatrix} q_0(x_1) & ‚ãØ & q_{n-1}(x_1) \\
                ‚ãÆ & ‚ã± & ‚ãÆ \\
                q_0(x_n) & ‚ãØ & q_{n-1}(x_n) \end{bmatrix}
$$
Note that if $p(x) = [q_0(x) | ‚ãØ | q_{n-1}(x)] ùêú$ then
$$
\begin{bmatrix}
p(x_1) \\
‚ãÆ \\
p(x_n)
\end{bmatrix} = VÃÉ ùêú
$$
But we see that (similar to the Fourier case)
$$
Q_n^w VÃÉ = \begin{bmatrix} Œ£_n^w[q_0 q_0] & ‚ãØ & Œ£_n^w[q_0 q_{n-1}]\\
                ‚ãÆ & ‚ã± & ‚ãÆ \\
                Œ£_n^w[q_{n-1} q_0] & ‚ãØ & Œ£_n^w[q_{n-1} q_{n-1}]
                \end{bmatrix} = I_n
$$

‚àé

**Corollary** Gaussian quadrature is an interpolatory quadrature rule.



A consequence of being an interpolatory quadrature rule is that it is exact for all
polynomials of degree $n-1$. The _miracle_ of Gaussian quadrature is it is exact for twice
as many!



**Theorem (Exactness of Gauss quadrature)** If $p(x)$ is a degree $2n-1$ polynomial then
Gauss quadrature is exact:
$$
‚à´_a^b p(x)w(x) {\rm d}x = Œ£_n^w[p].
$$

**Proof**
Using polynomial division algorithm (e.g. by matching terms) we can write
$$
p(x) = q_n(x) s(x) + r(x)
$$
where $s$ and $r$ are degree $n-1$. Then we have:
$$
Œ£_n^w[p] = \undebrace{Œ£_n^w[q_n s]}_{$0$ since evaluating $q_n$ at zeros} + Œ£_n^w[r] = ‚à´_a^b r(x) w(x) {\rm d}x
= \underbrace{‚à´_a^b q_n(x)s(x) w(x) {\rm d}x}_{$0$ since $s$ is degree $<n$}  + ‚à´_a^b r(x) w(x) {\rm d}x = ‚à´_a^b p(x)w(x) {\rm d}x.
$$
‚àé

